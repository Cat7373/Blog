---
title: LeetCode 刷题笔记
date: 2020-02-29 19:13:00
categories: LeetCode
tags:
  - LeetCode
---
<!-- 20200229 -->
# 前言
此文章用于在阅读[labuladong的算法小抄](https://labuladong.gitbook.io/algo/)时记录笔记，未来可能也会用于其它算法和数据结构方面学习的笔记

<!-- more -->
> 文中实例代码由`Java`或`Kotlin`编写，其中，原文中的代码可能会直接 copy 或转为`Java`后 copy 过来，我自己写的代码多为`Kotlin`

# 第零章 必读
## 数据结构的存储方式
计算机中，数据结构只有两种：数组（顺序存储）和链表（链式存储）

其它数据结构均是由这两种结构进行组合得来

队列和栈既可以使用数组也可以使用链表来实现，数组需要考虑扩容问题；链表则不用，但需要更多空间存储指针

图也可以用这两种结构来表示，邻接表是链表，邻接矩阵就是而为数组，邻接矩阵判断连通性迅速，但用于存储稀疏图会浪费空间；邻接表比较节省空间，但操作效率比邻接矩阵要低

散列(Hash)表是通过散列函数将键映射到一个大数组里，对于散列冲突，拉链法会通过链表依次存储同散列的键(如 Java 的 HashMap)，线性探查法会存到后面的空位上，不需要指针存储空间，但操作稍微复杂

树，用数组实现就是堆，因为堆是一个完全二叉树；用链表实现就是常见的树，不一定是完全二叉树，在基于链表的树的结构之上，又衍生出各种设计，如：二叉搜索树、AVL 树、红黑树、区间树、B 树等等，对应解决不同问题

数组是紧凑连续的存储结构，可以随机访问，且较节约存储空间；但如果内存一次没分配够，需要进行扩容，就需要分配一块更大的空间，将数据全部 copy 过去，时间复杂度 O(N)；如果你想在数组中间插入或删除，就必须移动后面的数据以保持连续，时间复杂度也是 O(N)

链表元素并不连续，是靠指针指向下一个元素的位置的，因此不存在扩容问题；如果知道某一个元素的先驱和后驱，操作指针即可删除或插入新元素，时间复杂度 O(1)，但正因为存储空间不连续，因此不能随机访问，而且由于需要存储指针，因此会相对消耗更多存储空间

## 数据结构的基本操作
对任何数据结构，其基本操作无非是遍历 + 访问，具体一点就是增删改查

数据结构种类很多，但他们的目的都是在不同场景下，尽可能高效的进行增删改查

数据访问常见的几种框架：

```java
// 数组遍历结构，典型的线性迭代结构
void traverse(int[] arr) {
    for (int i = 0; i < arr.length; i++) {
        // 访问 arr[i]
    }
}
```

```java
// 链表遍历框架
/** 基本单链表节点 */
class Node {
    int val,
    Node next;
}

// 迭代访问
void traverse(Node head) {
    for (Node p = head; p != null; p = p.next) {
        // 访问 p.val
    }
}

// 递归访问
void traverse(Node head) {
    // 访问 head.val
    if (head.next != null) {
        traverse(head.next)
    }
}
```

```java
// 二叉树遍历框架，典型的非线性递归遍历结构
/** 基本的二叉树节点 */
class Node {
    int val;
    Node left, right;
}

void traverse(Node root) {
    // 访问 root.val
    if (root.left != null) {
        traverse(root.left)
    }
    if (root.right != null) {
        traverse(root.right)
    }
}
```

```java
// N 叉树遍历
/** N 叉树节点 */
class Node {
    int val;
    Node[] children;
}

void traverse(Node root) {
    // 访问 root.val
    for (Node child : root.children) {
        traverse(child)
    }
}
```

<!-- 20200301 -->
# 第一章 动态规划
动态规划问题一般形式就是求最值。动态规划其实是运筹学的一种最优化方法，只不过在计算机问题上应用比较多，比如求最长递增子序列、最小编辑距离等

既然是求最值，核心问题是什么呢？核心问题就是穷举。因为要求最值，就要把所有可行的答案穷举处理啊，然后在其中找最值

动态规划的穷举有些特别，这类问题存在`重叠子问题`，如果暴力穷举的话效率会很低，所以需要`备忘录`或`DP table`来优化穷举过程，避免不必要的计算

另外，虽然动态规划的核心思想就是穷举求最值，但问题可以千变万化，穷举所有可行解并不是一件容易的事，只有列出正确的`状态转移方程`才能正确的穷举

## 斐波那契数列
> 尽管这个列子严格来说并不是动态规划问题，但可助于理解重叠子问题

### 暴力递归
斐波那契数列的数学形式递归是这样的：

```kotlin
fun fib(n: Int) = when(n) {
    1, 2 -> 1
    else -> fib(n - 1) + fib(n - 2)
}
```

学校老师讲递归的时候经常会拿这个举例，虽然这样的代码简洁易懂，但十分低效，比如想要计算 f(20)，就得先计算 f(19) 和 f(18)，然后要计算 f(19)，就要先计算 f(18) 和 f(17)，以此类推，最后到 f(1) 或 f(2) 时，结果已知，就能直接返回结果，递归树就不再向下生长了

递归算法的时间复杂度怎么计算？子问题个数乘以解决一个子问题需要的时间

子问题个数，显然次问题中为指数级别，子问题个数为 O(2^n)

解决一个子问题需要的时间，由于本算法没有循环，只有一个加法操作，时间为 O(1)

所以这个算法的时间复杂度为 O(2^n)，指数级别，爆炸

观察递归流程，很明显可以发现算法低效的原因：存在大量重复计算，如 f(18) 被计算了两次，而且由于 f(18) 的子问题体量巨大，多算一遍会耗费巨大的时间。更何况，还不止 f(18) 这一个问题被重复计算，所以这个算法及其低效

这就是动态规划问题的第一个性质：重叠子问题

### 带备忘录的递归
既然耗时的原因是重复计算，那么我们可以造一个`备忘录`，每次算出某个子问题的答案后先记到备忘录里再返回，每次遇到一个子问题先去`备忘录`查一查，如果发现问题已经被解决过，拿就把答案直接拿出来用，不要再重复计算一次了。

```kotlin
fun fib(n: Int) = fibHelper(IntArray(n), n) // 初始化一个备忘录，转给 helper 方法

fun fibHelper(memo: IntArray, n: Int) = when(n) {
    // base case
    1, 2 -> 1
    else -> {
        var v = memo[n - 1] // 取出备忘录中的结果
        if (v != 0) {
            v // 已经计算过
        } else {
            // 进行计算
            v = fibHelper(memo, n - 1) + fibHelper(memo, n - 2)
            memo[n - 1] = v // 保存到备忘录中
            v // 返回结果
        }
    }
}
```

这样，每个子问题就只会被计算一次，第二次就可以直接从`备忘录`中获取结果，实际上，带`备忘录`的递归算法，把一棵存在巨量冗余的递归树通过`剪枝`，改造成了一副不存在冗余的递归图，极大的减少了子问题的个数

递归算法的时间复杂度怎么计算？子问题个数乘以解决一个子问题需要的时间

子问题个数，由于本算法不存在冗余计算，子问题就是 f(1)、f(2)、f(3) ... f(19)、f(20)，数量与输入规模 n = 20 成正比，所以子问题个数为 O(n)

解决一个子问题的时间，同上，没什么循环，因此为 O(1)

所以，本算法的时间复杂度为 O(n)，对比暴力递归的 O(2^n)，简直是降维打击

实际上，这种解法的效率和迭代的动态规划已经差不多了，只不过这种方法叫`自顶向下`，动态规划叫做`自底向上`

自顶向下是指从一个规模较大的问题如 f(20) 向下逐渐分解规模，直到 f(1)、f(2) 触底，然后逐层返回答案

自底向上是指，我们从最底下，最简单，问题规模最小的 f(1) 和 f(2) 开始往上推，直到推到我们想要的答案 f(20)，这就是动态规划的思路

### dp 数组的迭代解法
有了上一步的启发，我们可以把这个`备忘录`独立出来成为一张表，就叫`DP table`吧，在这张表上完成`自底向上`的推算

```kotlin
fun fib(n: Int): Int {
    val dp = IntArray(n)
    dp[0] = 1
    dp[1] = 1
    for (i in 2 until n) {
        dp[i] = dp[i - 1] + dp[i - 2]
    }
    return dp[n - 1]
}
```

你会发现这个`DP table`特别像之前那个剪枝后的结果，只是反过来算而已。实际上，带`备忘录`的递归解法终端鹅备忘录，最终完成后就是这个`DP tabel`，所以说这两种解法其实是差不多的，大部分情况下，效率也基本相同 <!-- 由于递归开销，其实备忘录的会慢一些 -->

这里，引出`状态转移方程这个名词`，实际上就是描述问题结构的数学形式：

![](/upload/leetcode_00001.png) <!-- 尝试用 hexo-math 代替？http://stevenshi.me/2017/06/26/hexo-insert-formula/ -->

你会发现，上面几种解法的所有操作，例如`f(n - 1) + f(n - 2)`、`dp[i] = dp[i - 1] + dp[i - 2]`，其实都是围绕这个方程式的不同表现形式，可见列出`状态转移方程`的重要性，它是解决问题的核心。很容易发现，其实状态转移方程直接代表着暴力解法

千万不要看不起暴力解，动态规划问题最困难的就是列出`状态转移方程`，即这个暴力解，优化方法无非是用`备忘录`或`DP table`，再无奥妙可言

最后，其实根据斐波那契数列的状态转移方程，当前状态只与前两个状态有关，所以其实并不需要那么长的一个`DP table`来存储所有的状态，只要想办法存储之前的两个状态就行了，所以，可以进一步优化，将空间复杂度降为 O(1)

```kotlin
fun fib(n: Int) = when(n) {
    1, 2 -> 1
    else -> {
        var prev = 1
        var curr = 1
        for (i in 3..n) {
            val sum = prev + curr
            prev = curr
            curr = sum
        }
        return curr
    }
}
```

<!-- 20200302 -->
## 凑零钱问题

